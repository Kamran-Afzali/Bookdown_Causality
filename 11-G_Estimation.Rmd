# G-Computation for Causal Inference

## Introduction

A 55-year-old patient with hypertension visits your clinic. She's currently on first-line antihypertensive medication, and you're considering adding a statin for cardiovascular protection. But the decision isn't straightforward—you need to consider not just the direct effect of the statin, but also how it might interact with her blood pressure medication, how her treatment adherence might change, and what her cardiovascular risk would look like over the next decade under different treatment strategies. Traditional clinical trial results provide average effects for populations that may not match your patient's complex profile and treatment history.

This is where G-computation, one of the most powerful yet underutilized tools in causal inference, becomes invaluable. Developed by Jamie Robins in the 1980s, G-computation provides a general framework for estimating causal effects from observational data by explicitly modeling the data-generating process and using those models to simulate counterfactual outcomes under different treatment regimes. Unlike propensity score methods that focus on modeling treatment assignment, G-computation directly models the outcome process, enabling estimation of effects in complex longitudinal settings with time-varying treatments and confounders.

The method's name derives from the "G-formula," a mathematical expression that standardizes over the distribution of confounders to obtain marginal causal effects. While the theory can appear daunting, the core intuition is remarkably straightforward: if we can build good models of how treatments affect outcomes while accounting for confounding, we can use those models to predict what would have happened under different treatment strategies. By comparing predictions under different scenarios, we recover causal effects without requiring the strong parametric assumptions of traditional regression adjustment.

G-computation excels in several scenarios where other causal methods struggle. It handles time-varying treatments where treatment decisions at one time point depend on previous outcomes, a common pattern in chronic disease management where clinicians adjust medications based on evolving patient status. The method naturally accommodates mediation analysis by allowing explicit modeling of intermediate variables on the causal pathway between treatment and outcome. It provides estimates of both conditional effects for specific patient profiles and marginal effects averaged over population characteristics, offering flexibility that matches diverse research questions. Perhaps most importantly, G-computation enables estimation of effects under complex dynamic treatment regimes that would be impossible or unethical to study in randomized trials.

The theoretical foundation of G-computation rests on the potential outcomes framework and Pearl's do-calculus, providing a rigorous basis for translating observational associations into causal statements. Consider a simple setting with treatment $A$, outcome $Y$, and baseline confounders $L$. Under the potential outcomes framework, each individual has potential outcomes $Y^{a}$ representing what their outcome would be if they received treatment level $a$. The causal effect of treatment versus control is $\mathbb{E}[Y^{1} - Y^{0}]$, but we face the fundamental problem of causal inference: we only observe $Y^{A}$, the outcome under the treatment actually received.

The G-formula provides the key to identification under exchangeability, also known as unconfoundedness or no unmeasured confounding. This assumption states that conditional on measured confounders $L$, treatment assignment is independent of potential outcomes: $Y^{a} \perp A \mid L$ for all $a$. Additionally, we require positivity, ensuring that every individual has positive probability of receiving each treatment level given their confounder values: $P(A = a \mid L = l) > 0$ for all relevant $a$ and $l$. Under these assumptions, the G-formula expresses the marginal mean of the potential outcome as $\mathbb{E}[Y^{a}] = \sum_{l} \mathbb{E}[Y \mid A = a, L = l] \cdot P(L = l)$.

This formula has an elegant interpretation: to compute the average outcome if everyone received treatment $a$, we first calculate the expected outcome for each confounder pattern under treatment $a$, then average over the actual distribution of confounders in the population. This standardization removes confounding by making treatment groups comparable with respect to confounder distributions. The causal effect becomes the difference between these standardized means under different treatment values: $\mathbb{E}[Y^{1}] - \mathbb{E}[Y^{0}]$.

The G-formula extends naturally to longitudinal settings with time-varying treatments and confounders, where treatment history affects future confounders which in turn affect future treatment decisions. This creates time-dependent confounding that violates the assumptions of standard regression adjustment. Consider a sequence of treatment decisions $A_0, A_1, \ldots, A_K$ and time-varying confounders $L_0, L_1, \ldots, L_K$ measured before each treatment decision, with final outcome $Y$ measured at time $K+1$. The longitudinal G-formula for a treatment regime $\bar{a} = (a_0, a_1, \ldots, a_K)$ becomes

$$\mathbb{E}[Y^{\bar{a}}] = \sum_{\bar{l}} \mathbb{E}[Y \mid \bar{A} = \bar{a}, \bar{L} = \bar{l}] \prod_{k=0}^{K} P(L_k = l_k \mid \bar{A}_{k-1} = \bar{a}_{k-1}, \bar{L}_{k-1} = \bar{l}_{k-1})$$

This formidable expression captures the sequential nature of longitudinal data where past treatments and confounders affect future covariate evolution. The formula marginalizes over all possible confounder trajectories, weighting each by its probability under the treatment regime of interest. While exact calculation requires summing over all possible covariate histories (computationally infeasible in practice), we can approximate this expectation through parametric modeling and Monte Carlo simulation.

The parametric G-formula algorithm translates the mathematical formula into a practical computational procedure through three essential steps: model specification, Monte Carlo simulation, and effect estimation. This approach requires specifying parametric models for how treatments affect outcomes and how treatments and past history affect future confounders, then using these models to simulate counterfactual trajectories under different treatment regimes.

In the model specification phase, we construct separate regression models for each time-varying component. For the outcome, we model $\mathbb{E}[Y \mid \bar{A}, \bar{L}]$ as a function of treatment history and confounder history, typically using generalized linear models appropriate to the outcome type. For each time-varying confounder $L_k$, we model $P(L_k \mid \bar{A}_{k-1}, \bar{L}_{k-1})$ capturing how previous treatments and covariates affect future covariate values. These models embody our understanding of the data-generating process and need not be correct in all details—they must simply capture the relationships between treatments, confounders, and outcomes accurately enough to provide unbiased effect estimates.

The Monte Carlo simulation phase generates counterfactual datasets under different treatment regimes. Starting with the observed baseline covariates $L_0$ for each individual, we simulate forward in time by first assigning the specified treatment value $a_0$, then using the fitted confounder models to simulate $L_1$ given $a_0$ and $L_0$, continuing this process through all time points. For each treatment regime of interest, this process generates a complete counterfactual dataset where every individual follows the same treatment strategy. By simulating the natural evolution of confounders under each treatment regime, we create synthetic comparison groups that would be exchangeable in a randomized trial.

Finally, we estimate the outcome under each regime by fitting the outcome model to each simulated dataset and averaging predictions across all individuals. The causal effect emerges from comparing these regime-specific outcome estimates. Confidence intervals accounting for both sampling variability and model estimation uncertainty require bootstrap resampling, where we repeat the entire procedure on bootstrap samples of the original data to quantify estimation uncertainty.

## Case Study: Hypertension Management Over Time

We'll explore G-computation through a realistic scenario involving management of hypertension with time-varying treatment intensity and evolving cardiovascular risk. Our goal is to estimate the effect of sustained intensive blood pressure control versus standard control on five-year cardiovascular event risk, accounting for treatment adjustments based on evolving patient status.


This simulation captures realistic features of hypertension management including treatment intensification based on blood pressure levels and risk factors, imperfect adherence that affects treatment effectiveness, blood pressure evolution depending on treatment and adherence patterns, and cardiovascular events influenced by current blood pressure, treatment, and baseline risk factors. The time-dependent confounding emerges naturally as blood pressure at time $t$ affects treatment decisions at time $t$ and outcomes at time $t+1$, while past treatment affects current blood pressure.

### Fitting the G-Formula Model

The `gfoRmula` package provides a comprehensive implementation of the parametric G-formula algorithm with built-in bootstrap inference and diagnostic tools. The key challenge lies in correctly specifying the models for time-varying confounders and the outcome, capturing the dependence structure while avoiding overfitting.


The model specification requires careful consideration of which variables predict confounder evolution and outcomes. For systolic blood pressure, we include the lagged value to capture persistence, current treatment and adherence to capture treatment effects, and baseline risk factors that might influence blood pressure trajectory. The adherence model captures how treatment intensity and perceived risk (via lagged blood pressure) influence compliance. The outcome model includes both current blood pressure and treatment, allowing the effect of treatment to operate both through blood pressure reduction and potentially through other pathways.

### Interpreting Results and Model Diagnostics

The G-formula estimates reveal the causal effects of sustained treatment strategies by comparing counterfactual risks under different regimes. The key estimands include risk under each treatment regime representing the probability of cardiovascular events by five years if all patients followed that regime, risk differences quantifying absolute risk reduction from intensive treatment, and risk ratios providing relative measures of treatment benefit.

Beyond point estimates, we must evaluate model adequacy through diagnostic procedures. The `gfoRmula` package provides tools for assessing whether our parametric models adequately capture the data-generating process. Goodness-of-fit statistics for each covariate model indicate whether we correctly specified the functional form and included relevant predictors. Balance checks compare the simulated covariate distributions under the natural course to the observed distributions—large discrepancies suggest model misspecification.

Good agreement between observed and simulated blood pressure trajectories under the natural course provides evidence that our covariate models adequately capture the time-varying confounder process. Substantial deviations would suggest model misspecification requiring revision of functional forms, addition of interaction terms, or inclusion of additional predictors. The confidence intervals around regime-specific risks reflect bootstrap uncertainty, accounting for both sampling variability and estimation error in the fitted models.

### Advanced Applications: Dynamic Treatment Regimes

The true power of G-computation emerges when estimating effects of dynamic treatment regimes that adapt to patient status over time, mimicking realistic clinical decision-making where treatment intensification depends on evolving patient characteristics. Unlike static regimes that specify treatment at baseline, dynamic regimes specify treatment rules as functions of observed history.


Dynamic regimes enable answering clinically relevant questions such as whether treating based on blood pressure thresholds versus treating all patients yields better outcomes, what threshold should trigger treatment intensification for different patient subgroups, and how frequently monitoring should occur to implement threshold-based treatment rules effectively. The G-formula naturally accommodates such questions by simulating how blood pressure would evolve under different decision rules and computing resulting event rates.

## Practical Considerations and Limitations

Successful G-computation implementation requires attention to several practical considerations that determine whether the method provides valid causal estimates. Model specification represents the most critical challenge, as the method depends on correctly modeling the relationships between treatments, confounders, and outcomes. Unlike propensity score methods that can be robust to outcome model misspecification, G-computation requires accurate outcome and covariate models. However, this requirement is less stringent than it appears—models need not be perfectly specified, but must capture the essential relationships between variables. Including relevant interactions, nonlinear terms, and lagged variables typically suffices for adequate performance.

Sample size considerations become particularly important in longitudinal settings where we must fit separate models for each time-varying covariate. Small samples may not provide stable estimates, especially when treatment patterns are sparse or covariate distributions change substantially over time. As a rough guideline, longitudinal G-computation requires at least several hundred subjects with complete follow-up to provide reliable estimates, though exact requirements depend on the complexity of the data-generating process and number of time points.

The fundamental assumption of no unmeasured confounding cannot be empirically verified and requires substantive knowledge about the data-generating process. In observational hypertension studies, unmeasured factors like patient preferences, physician practice patterns, or health behaviors might influence both treatment decisions and cardiovascular outcomes. Sensitivity analyses examining how results change under various degrees of unmeasured confounding provide important context for causal conclusions, though G-computation itself does not resolve confounding from unmeasured variables.

Computational demands of bootstrap inference can become substantial in longitudinal settings with many time points and covariates. Each bootstrap iteration requires refitting all covariate models and outcome models, then simulating counterfactual trajectories for all subjects under each treatment regime. Parallel computing substantially reduces computation time, but researchers should expect hours rather than minutes for complex analyses with adequate bootstrap samples.

Model compatibility represents another consideration often overlooked in practice. When covariate models and outcome models make incompatible assumptions about the data-generating process, G-formula estimates may be biased even if individual models appear well-specified. For instance, if the blood pressure model assumes linear time trends but the outcome model includes nonlinear time effects, the simulated counterfactual trajectories may not accurately reflect how outcomes would evolve under different treatment regimes. Ensuring consistency across models requires careful consideration of functional forms and included covariates.

Recent methodological developments extend G-computation to increasingly complex settings that expand its practical applicability. Doubly robust estimation combines G-computation with inverse probability weighting, providing valid estimates if either the outcome model or the treatment model is correctly specified. This approach offers protection against model misspecification that purely model-based G-computation lacks, though at the cost of increased computational complexity and potentially wider confidence intervals.

Machine learning methods for G-computation replace parametric models with flexible algorithms like random forests, gradient boosting, or neural networks. These approaches can capture complex nonlinear relationships and interactions without requiring explicit specification, potentially improving performance when the true data-generating process differs substantially from parametric assumptions. However, machine learning G-computation requires careful attention to overfitting and may provide less stable estimates in moderate sample sizes, along with reduced interpretability compared to parametric approaches.

Targeted maximum likelihood estimation provides a more efficient alternative to standard G-computation by incorporating information from both the outcome and treatment models. This approach updates initial model-based estimates using clever weighting schemes that reduce bias and improve precision. The method has gained traction in epidemiology and biostatistics for settings where optimal efficiency matters, though implementation complexity exceeds standard G-computation.

Mediation analysis represents a natural application for G-computation, as the method's explicit modeling of intermediate variables enables decomposition of total effects into direct and indirect pathways. For hypertension management, we might decompose the treatment effect into a direct effect not mediated by blood pressure reduction and an indirect effect operating through blood pressure control. Such decompositions provide mechanistic insights about how treatments produce their effects, guiding development of more effective interventions.

## Conclusion: Integrating G-Computation into Practice

G-computation provides a powerful and flexible framework for causal inference from observational data, particularly excelling in longitudinal settings with time-varying treatments and confounders where traditional methods struggle. The method's explicit modeling of the data-generating process enables estimation of effects under complex treatment regimes while providing interpretable insights about how treatments affect outcomes over time.

Our hypertension case study demonstrates practical implementation using the `gfoRmula` package, showing how to specify appropriate models, simulate counterfactual trajectories, and interpret results in clinically meaningful terms. The ability to compare static and dynamic treatment regimes enables answering questions directly relevant to clinical decision-making about when and how intensively to treat patients with evolving risk profiles.

Successful application requires careful attention to model specification, adequate sample sizes, and validation of modeling assumptions through diagnostic procedures. The method works best when researchers possess substantive knowledge about the causal structure underlying their data and can specify models that capture essential relationships between treatments, confounders, and outcomes. While G-computation demands more upfront modeling effort than some alternatives, it rewards that investment with estimates of causal effects under realistic treatment strategies that match how clinicians actually make decisions in practice.

The integration of G-computation with modern computational tools and machine learning methods continues expanding the frontier of what's possible in causal inference from complex longitudinal data. As healthcare increasingly relies on observational electronic health records and registry data to inform treatment decisions, methods like G-computation that can extract valid causal conclusions from such data become essential tools for evidence-based medicine and precision healthcare delivery.